{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# **Fine-Tuning Stable Diffusion with Low-Rank Adaptation (LoRA)**\n",
    "\n",
    "## Project Overview\n",
    "This project explores parameter-efficient fine-tuning of Stable Diffusion models using Low-Rank Adaptation (LoRA) on the Flickr8k dataset. We compare two different approaches:\n",
    "\n",
    "1. **Model 1 (Minimal)**: Training on 100 images with attention-only LoRA adaptation\n",
    "2. **Model 2 (Comprehensive)**: Training on 3,000 images with expanded LoRA targeting both attention and feedforward networks\n",
    "\n",
    "Our goal is to demonstrate how LoRA enables efficient adaptation of large diffusion models even with limited computational resources, and to analyze the impact of dataset size and architecture choices on generation quality.\n",
    "\n",
    "## Theoretical Background\n",
    "### Stable Diffusion\n",
    "Stable Diffusion is a latent text-to-image diffusion model capable of generating photo-realistic images from text prompts. The model works by:\n",
    "1. Encoding the input text using CLIP text encoder\n",
    "2. Applying a diffusion process in the latent space\n",
    "3. Gradually denoising random noise into a coherent image guided by the text embedding\n",
    "\n",
    "### Low-Rank Adaptation (LoRA)\n",
    "LoRA is a parameter-efficient fine-tuning technique that:\n",
    "- Freezes pre-trained model weights\n",
    "- Injects trainable rank decomposition matrices into layers\n",
    "- Approximates weight updates using low-rank matrices: ΔW = AB, where A ∈ ℝᵐˣʳ, B ∈ ℝʳˣⁿ\n",
    "- Reduces trainable parameters by ~97% compared to full fine-tuning\n",
    "\n",
    "LoRA enables adaptation of large models on consumer hardware while maintaining performance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Fine-Tune Stable Diffusion with Flickr8k**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Import Libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "from transformers import CLIPTokenizer\n",
    "from diffusers import StableDiffusionPipeline, DDPMScheduler\n",
    "from peft import get_peft_model, LoraConfig\n",
    "from datasets import Dataset as HFDataset\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### **Configuration parameters for model fine-tuning**\n",
    "\n",
    "**1. image_dir: Directory containing Flickr8k images**\n",
    "\n",
    "**2. captions_file: Path to captions dataset**\n",
    "\n",
    "**3. pretrained_model: Base Stable Diffusion model to fine-tune**\n",
    "\n",
    "**4. output_dir: Where to save the fine-tuned model**\n",
    "\n",
    "**5. image_size: Resolution for training images (512x512)**\n",
    "\n",
    "**6. batch_size: Batch size for training (larger for Model 2)**\n",
    "\n",
    "**7. num_epochs: Number of training epochs**\n",
    "\n",
    "**8. lr: Learning rate (lower for Model 2 for stability)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_dir = \"C:/Users/molavade.s/Latent_Diffusion_model/flickr8k/Images\"\n",
    "captions_file = \"C:/Users/molavade.s/Latent_Diffusion_model/flickr8k/captions.txt\"\n",
    "pretrained_model = \"CompVis/stable-diffusion-v1-4\"\n",
    "output_dir = \"./sd-fine-tuned-lora-updated\"\n",
    "image_size = 512\n",
    "batch_size = 4\n",
    "num_epochs = 20\n",
    "lr = 5e-6\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Load Captions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_captions(captions_path):\n",
    "    pairs = []\n",
    "    with open(captions_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if not line or line.startswith(\"image,caption\"):\n",
    "                continue\n",
    "            try:\n",
    "                img, caption = line.split(',', 1)\n",
    "                img = img.split('#')[0]\n",
    "                full_img_path = os.path.join(image_dir, img)\n",
    "                if os.path.exists(full_img_path):\n",
    "                    pairs.append({'image': full_img_path, 'caption': caption.strip()})\n",
    "                else:\n",
    "                    print(f\"Image file not found: {full_img_path}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing line: {line} – {e}\")\n",
    "    return pairs\n",
    "\n",
    "\n",
    "# Load and convert to HuggingFace Dataset\n",
    "pairs = load_captions(captions_file)[:3000]  # Limit to first 1000\n",
    "from datasets import Dataset as HFDataset\n",
    "hf_dataset = HFDataset.from_list(pairs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Tokenizer and Transform** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = CLIPTokenizer.from_pretrained(\"openai/clip-vit-large-patch14\")\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((image_size, image_size)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.5], [0.5])\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Custom Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FlickrDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    def __getitem__(self, idx):\n",
    "        example = self.data[idx]\n",
    "        image = Image.open(example['image']).convert('RGB')\n",
    "        pixel_values = transform(image)\n",
    "        text_inputs = tokenizer(example['caption'], padding='max_length', truncation=True, max_length=77, return_tensors='pt')\n",
    "        return {\n",
    "            'pixel_values': pixel_values,\n",
    "            'input_ids': text_inputs.input_ids.squeeze(0),\n",
    "            'attention_mask': text_inputs.attention_mask.squeeze(0)\n",
    "        }\n",
    "\n",
    "dataset = FlickrDataset(hf_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Load Pipeline and Freeze**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = StableDiffusionPipeline.from_pretrained(pretrained_model, torch_dtype=torch.float16 if device==\"cuda\" else torch.float32)\n",
    "pipe.to(device)\n",
    "pipe.vae.requires_grad_(False)\n",
    "pipe.text_encoder.requires_grad_(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"to_q\", \"to_k\", \"to_v\", \"to_out.0\", \"ff.net.0.proj\", \"ff.net.2\"],\n",
    "    bias=\"none\"\n",
    ")\n",
    "\n",
    "pipe.unet = get_peft_model(pipe.unet, lora_config)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(pipe.unet.parameters(), lr=lr)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "pipe.scheduler = DDPMScheduler.from_config(pipe.scheduler.config)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    pipe.unet.train()\n",
    "    for batch in tqdm(dataloader, desc=f'Epoch {epoch+1}/{num_epochs}'):\n",
    "        images = batch['pixel_values'].to(device, dtype=torch.float16)\n",
    "        latents = pipe.vae.encode(images).latent_dist.sample() * 0.18215\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        noise = torch.randn_like(latents)\n",
    "        timesteps = torch.randint(0, pipe.scheduler.config.num_train_timesteps, (latents.shape[0],), device=device).long()\n",
    "        noisy_latents = pipe.scheduler.add_noise(latents, noise, timesteps)\n",
    "        with torch.no_grad():\n",
    "            encoder_hidden_states = pipe.text_encoder(input_ids)[0].to(dtype=torch.float16)\n",
    "        model_pred = pipe.unet(noisy_latents, timesteps, encoder_hidden_states).sample\n",
    "        loss = torch.nn.functional.mse_loss(model_pred, noise)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(f'Epoch {epoch+1}: Loss = {loss.item()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Save final LoRA fine-tuned UNet**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-tuned U-Net saved to: ./sd-fine-tuned-lora-updated\n"
     ]
    }
   ],
   "source": [
    "pipe.unet.save_pretrained(output_dir)\n",
    "print(f\"Fine-tuned U-Net saved to: {output_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers import StableDiffusionPipeline\n",
    "import torch\n",
    "from peft import PeftModel, LoraConfig\n",
    "\n",
    "# Load original SD pipeline\n",
    "pipe = StableDiffusionPipeline.from_pretrained(\"CompVis/stable-diffusion-v1-4\", torch_dtype=torch.float16)\n",
    "pipe.to(\"cuda\")\n",
    "\n",
    "# Load your LoRA fine-tuned U-Net\n",
    "from peft import PeftModel\n",
    "pipe.unet = PeftModel.from_pretrained(pipe.unet, \"./sd-fine-tuned-lora-updated\")\n",
    "pipe.unet.eval()\n",
    "\n",
    "# Enable faster generation\n",
    "pipe.enable_attention_slicing()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions\n",
    "\n",
    "1. LoRA enables efficient fine-tuning of large diffusion models\n",
    "2. Even modest datasets (100-3,000 samples) can produce meaningful adaptations\n",
    "3. Strategic choices in LoRA configuration significantly impact results\n",
    "4. The approach scales well from small experiments to larger training sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myenv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
